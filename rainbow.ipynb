{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "import tqdm.autonotebook as auto\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from rainbow.rainbow import Rainbow\n",
    "from rainbow.models import NoisyDistNet, ConvDQN\n",
    "from rainbow.utils import choose_max, plot_var_history, get_model_name, preprocess_binary\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gym_env(Environment):\n",
    "    def make_env():\n",
    "        return gym.make(Environment)\n",
    "    return make_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_check_point(make_env, act_freq, network, process_obs,\n",
    "                     check_pts, save_path, atoms=None, trials=10):\n",
    "    env = make_env()\n",
    "    if atoms is None:\n",
    "        model = network(env.action_space.n)\n",
    "    else:\n",
    "        model = network(env.action_space.n, atoms)\n",
    "    avg_returns = []\n",
    "    for check_pt in check_pts:\n",
    "        state_dict = torch.load(save_path + f'/ep_{check_pt}/model_state.pt')\n",
    "        model.load_state_dict(state_dict)\n",
    "        avg_returns.append([])\n",
    "        for i in range(1, trials+1, 1):\n",
    "            env.seed(i)\n",
    "            obs = env.reset()\n",
    "            obs_stack = [obs]\n",
    "            done = False\n",
    "            time_step = 0\n",
    "            total_reward = 0\n",
    "            while not done:\n",
    "                if len(obs_stack) == act_freq or time_step == 0:\n",
    "                    act = choose_max(model.get_values(process_obs(obs_stack)))\n",
    "                    obs_stack = []\n",
    "                obs, reward, done, info = env.step(act)\n",
    "                obs_stack.append(obs)\n",
    "                total_reward += reward\n",
    "                time_step += 1\n",
    "            avg_returns[-1].append(total_reward)\n",
    "    env.close()\n",
    "    avg_returns = np.array(avg_returns).sum(axis=-1)\n",
    "    max_idx = np.argmax(avg_returns)\n",
    "    # get best performing model\n",
    "    state_dict = torch.load(save_path + f'/ep_{check_pts[max_idx]}/model_state.pt')\n",
    "    model.load_state_dict(state_dict)\n",
    "    print('Best performing model: Episode %d' % (check_pts[max_idx]))\n",
    "    for check_pt in check_pts:\n",
    "        shutil.rmtree(save_path + f'/ep_{check_pt}')\n",
    "    torch.save(model.state_dict(), save_path + '/' + 'model_state_dict')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_agent_vid(make_env, act_freq, network, process_obs,\n",
    "               save_path, atoms=None, trials=1):\n",
    "    env = make_env()\n",
    "    if atoms is None:\n",
    "        model = network(env.action_space.n)\n",
    "    else:\n",
    "        model = network(env.action_space.n, atoms)\n",
    "\n",
    "    state_dict = torch.load(save_path + '/' + 'model_state_dict')\n",
    "    model.load_state_dict(state_dict)\n",
    "    trial_returns = []\n",
    "    obs_array = []\n",
    "    for i in range(1, trials+1, 1):\n",
    "        print('Trial: %d' % (i))\n",
    "        time_step = 0\n",
    "        env.seed(i+2)\n",
    "        obs = env.reset()\n",
    "        obs_stack = [obs]\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            # env.render()\n",
    "            if (time_step %2 == 0):\n",
    "                img = env.render(mode='rgb_array')\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "                obs_array.append(img)   # capture video\n",
    "                # cv2.imwrite('video/'+str(time_step)+'.jpg', img)\n",
    "            time.sleep(0.01)\n",
    "            if len(obs_stack) == act_freq or time_step == 0:\n",
    "                act = choose_max(model.get_values(process_obs(obs_stack)))\n",
    "                obs_stack = []\n",
    "            obs, reward, done, info = env.step(act)\n",
    "            obs_stack.append(obs)\n",
    "            total_reward += reward\n",
    "            time_step += 1\n",
    "        print('Reward: %d' % total_reward)\n",
    "        trial_returns.append(total_reward)\n",
    "    env.close()\n",
    "    return obs_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_video(img_list, video_name='apple.avi'):\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v') \n",
    "    height, width = img_list[0].shape[:-1]\n",
    "    out = cv2.VideoWriter(video_name, fourcc, 17.5, (width, height))\n",
    "    for image in img_list:  \n",
    "        out.write(image)\n",
    "    out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_agent(make_env, act_freq, network, process_obs,\n",
    "               save_path, atoms=None, trials=1):\n",
    "    env = make_env()\n",
    "    if atoms is None:\n",
    "        model = network(env.action_space.n)\n",
    "    else:\n",
    "        model = network(env.action_space.n, atoms)\n",
    "\n",
    "    state_dict = torch.load(save_path + '/' + 'model_state_dict')\n",
    "    model.load_state_dict(state_dict)\n",
    "    trial_returns = []\n",
    "    for i in range(1, trials+1, 1):\n",
    "        print('Trial: %d' % (i))\n",
    "        env.seed(i)\n",
    "        obs = env.reset()\n",
    "        obs_stack = [obs]\n",
    "        done = False\n",
    "        time_step = 0\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            env.render()\n",
    "            time.sleep(0.01)\n",
    "            if len(obs_stack) == act_freq or time_step == 0:\n",
    "                act = choose_max(model.get_values(process_obs(obs_stack)))\n",
    "                obs_stack = []\n",
    "            obs, reward, done, info = env.step(act)\n",
    "            obs_stack.append(obs)\n",
    "            total_reward += reward\n",
    "            time_step += 1\n",
    "        print('Reward: %d' % total_reward)\n",
    "        trial_returns.append(total_reward)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(make_env, process_obs, act_freq,\n",
    "                   num_runs, num_episodes, agent_args,\n",
    "                   tb_path=None, watch_agent=True):\n",
    "    reward_history = []\n",
    "    agents_hist = []\n",
    "    env = make_env()\n",
    "    assert isinstance(env.action_space, gym.spaces.Discrete), \\\n",
    "        \"Action space is not discrete\"\n",
    "    act_dim = env.action_space.n\n",
    "    for i, agent_kwargs in enumerate(agent_args):\n",
    "        reward_history.append([])\n",
    "        # Start the runs for each setting\n",
    "        for run in range(1, num_runs+1):\n",
    "            reward_history[i].append([])\n",
    "            env.seed(run)\n",
    "            agent_kwargs[\"seed\"] = run\n",
    "            agent = Rainbow(act_dim, **agent_kwargs)\n",
    "            if tb_path:\n",
    "                writer = SummaryWriter(get_model_name(agent_kwargs, tb_path))\n",
    "            # Start the episodes\n",
    "            for episode in auto.tqdm(range(1, num_episodes+1),\n",
    "                                     desc='Config %d | Run %d ' % (i+1, run),\n",
    "                                     leave=False):\n",
    "                observation = env.reset()\n",
    "                if process_obs is not None:\n",
    "                    observation = process_obs([observation])\n",
    "                done = False\n",
    "                time_step = 0\n",
    "                action = agent.start(observation)\n",
    "                interim_reward = 0\n",
    "                interim_obs = []\n",
    "                # Start interaction with environment\n",
    "                while not done:\n",
    "                    # Take a step in the environment\n",
    "                    observation, reward, done, info = env.step(action)\n",
    "                    interim_reward += reward\n",
    "                    interim_obs.append(observation)\n",
    "                    time_step += 1\n",
    "                    if not done and time_step % act_freq == 0:\n",
    "                        # Get next action from agent\n",
    "                        if process_obs is not None:\n",
    "                            interim_obs = process_obs(interim_obs)\n",
    "                        action = agent.take_step(interim_reward, interim_obs)\n",
    "                        interim_reward = 0\n",
    "                        interim_obs = []\n",
    "                    elif done:\n",
    "                        episode_reward = agent.end(reward)\n",
    "                # post episode processing\n",
    "                reward_history[i][run-1].append(episode_reward)\n",
    "                if tb_path:\n",
    "                    writer.add_scalar('Episode Reward',\n",
    "                                      episode_reward, episode)\n",
    "                    if episode % 25 == 0:\n",
    "                        data = agent.get_train_data()\n",
    "                        if data is not None:\n",
    "                            writer.add_scalar('Loss', data.get('loss'), episode)\n",
    "            # Find the best performing checkpoint\n",
    "            agents_hist.append(best_check_point(make_env, act_freq,\n",
    "                                                agent_kwargs['n_net'], process_obs,\n",
    "                                                agent_kwargs['check_pts'],\n",
    "                                                agent.save_path,\n",
    "                                                agent.z_atoms.cpu()))\n",
    "            # View the agent's performance\n",
    "            if watch_agent:\n",
    "                view_agent(make_env, act_freq, agent_kwargs['n_net'],\n",
    "                           process_obs, agent.save_path,\n",
    "                           agent.z_atoms.cpu())\n",
    "            if tb_path:\n",
    "                writer.close()\n",
    "    env.close()\n",
    "    return reward_history, agents_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_cartpole(runs=1, episodes=150, render=True):\n",
    "    # Setup cartpole environment\n",
    "    Environment = 'CartPole-v1'\n",
    "    test_env = make_gym_env(Environment)\n",
    "\n",
    "    ag_args = [\n",
    "               {'n_step': 3,\n",
    "                'n_net': lambda act, atoms: NoisyDistNet(4, [64, 128, 64], act, atoms),\n",
    "                'policy_update_freq': 2, 'target_update_freq': 75,\n",
    "                'mini_batch': 32, 'discount': 0.999, 'replay_mem': 10000,\n",
    "                'lr': {'start': 1e-3, 'end': 1e-4, 'period': 5000},\n",
    "                'eps': 0, 'learn_start': 1e2,\n",
    "                'pri_buf_args': {'alpha': 0.7, 'beta': (0.5, 1), 'period': 1e6},\n",
    "                'distrib_args': {'atoms': 21, 'min_val': 0, 'max_val': 500},\n",
    "                'clip_grads': None,\n",
    "                'check_pts': [50, 100, 150], 'save_path': 'data/CartPole-v1',\n",
    "                'no_duel': False, 'no_double': False, 'no_priority_buf': False,\n",
    "                'no_noise': False, 'no_distrib': False},\n",
    "               ]\n",
    "\n",
    "    return run_experiment(make_env=test_env, act_freq=1,\n",
    "                          process_obs=lambda x: x[-1],\n",
    "                          num_runs=runs, num_episodes=episodes,\n",
    "                          agent_args=ag_args,\n",
    "                          tb_path='runs/CartPole-v1', watch_agent=render)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pong(runs=1, episodes=100, render=True):\n",
    "    # Setup pong environment\n",
    "    Environment = 'Pong-v0'\n",
    "    test_env = make_gym_env(Environment)\n",
    "\n",
    "    ag_args = [\n",
    "               {'n_step': 3,\n",
    "                'n_net': lambda act, atoms: ConvDQN(4, act, atoms),\n",
    "                'policy_update_freq': 4, 'target_update_freq': 1250,\n",
    "                'mini_batch': 32, 'discount': 0.99, 'replay_mem': 250000,\n",
    "                'lr': {'start': 5e-4, 'end': 2.5e-4, 'period': 10000},\n",
    "                'eps': 0,\n",
    "                'pri_buf_args': {'alpha': 0.7, 'beta': (0.5, 1), 'period': 1e6},\n",
    "                'distrib_args': {'atoms': 21, 'min_val': -25, 'max_val': 25},\n",
    "                'clip_grads': 20, 'learn_start': 1e4,\n",
    "                'check_pts': [i*1000 for i in range(1, 100, 1)],\n",
    "                'save_path': 'data/Pong-v0/',\n",
    "                'no_duel': False, 'no_double': False,\n",
    "                'no_priority_buf': False, 'no_noise': False,\n",
    "                'no_distrib': False},\n",
    "                ]\n",
    "\n",
    "    return run_experiment(make_env=test_env, act_freq=4,\n",
    "                          process_obs=lambda x: preprocess_binary(x, 4, True),\n",
    "                          num_runs=runs, num_episodes=episodes,\n",
    "                          agent_args=ag_args,\n",
    "                          tb_path='runs/Pong-v0', watch_agent=render)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_cartpole()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# img_array = save_agent_vid(make_gym_env('Pong-v0'), 4, lambda act, atoms: ConvDQN(4, act, atoms),\n",
    "#                    lambda x: preprocess_binary(x, 4, True), '../', atoms=torch.zeros(21), trials=1)\n",
    "\n",
    "# make_video(img_array, 'pong.avi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# img_array = save_agent_vid(make_gym_env('CartPole-v1'), 1, lambda act, atoms: NoisyDistNet(4, [64, 128, 64], act, atoms),\n",
    "#                    lambda x: x[-1], '../trained_models/CartPole-v1', atoms=torch.zeros(21), trials=1)\n",
    "\n",
    "# make_video(img_array, 'cartpole.avi')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('mre': conda)",
   "language": "python",
   "name": "python38564bitmreconda0ee5b04cb063499ca4ff35c8e540568d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}